{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fb77f3",
   "metadata": {},
   "source": [
    "# 🌊 Deep Dive Into Isolation Forest\n",
    "> **작성자 : 오수지 (2022020660)**\n",
    "\n",
    "안녕하세요, 고려대학교 산업경영공학부 DSBA 연구실 석사과정 오수지입니다.  \n",
    "이번 노트북 튜토리얼에선 Model-based Anomaly Detection 방법론, 그 중에서도 **`Isolation Forest`**에 대해 코드를 통해 알아볼 예정입니다. (🔎 Anomaly Detection에 대한 기본적인 내용은 [유튜브 튜토리얼 영상](https://youtu.be/XshinhpMrLQ)을 참고해주세요.)\n",
    "\n",
    "Isolation Forest는 매우 직관적이고, 이상치 탐지 대회에서 아직까지도 많이 사용되고 있는 방법론인데요! 여러 개의 Decision Tree를 지속적으로 분기시키면서 모든 데이터 관측치의 고립 정도 여부에 따라 이상치를 판별하는 방법입니다. 쉽게 생각해보자면 이상치 데이터일수록 다른 정상 데이터와 다른 특성을 띌테니 빨리 고립될 것이고, 정상 데이터일수록 서로서로 비슷비슷할테니 늦게 고립되겠죠?! 즉, **특정 데이터가 고립되는 leaf node까지의 거리**를 `Anomaly Score`로 정의하고, 일찍 고립될수록, 즉 root node로부터 leaf node까지의 평균 거리가 짧을수록 anomaly score가 높아지게 됩니다.\n",
    "\n",
    "근데 수업을 들으면서도, 유튜브 동영상을 만들면서도 전 한 가지 의문점이 들었습니다. 분기의 기준이 되는 Attribute를 랜덤하게 선택하지 않으면 어떨까요?  Isolation Forest는 Decision Tree를 기반으로 하는 방법론이고, Decision Tree의 분기 기준은 Information gain, Gini Impurity, Chi-Square 등 여러가지가 있는만큼 만약 랜덤하게 하지 않으면 결과가 달라지지 않을까요?\n",
    "\n",
    "그러기 위해서 일단 (1) Isolation Forest를 Scratch부터 구현한 레포를 pseudo code와 대조해가며 line by line으로 이해를 하고, (2) Information Gain을 기준으로 분기하는 코드를 추가한 다음 (3) 두 방식간 성능 차이를 확인해보겠습니다 😀\n",
    "\n",
    "<img src=\"images/if.png\" width=\"550\">\n",
    "\n",
    "**📌 이번 튜토리얼의 목표**\n",
    ">1. Isolation Forest from Scratch\n",
    ">2. Isolation Forest without Randomness from Scratch\n",
    ">3. 1, 2번 간 성능 비교\n",
    "\n",
    "**📌 Reference**\n",
    "- https://github.com/mgckind/iso_forest\n",
    "- https://github.com/sahandha/eif\n",
    "- https://hongl.tistory.com/150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcee65",
   "metadata": {},
   "source": [
    "## 🛠 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e05b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import easydict\n",
    "import wandb\n",
    "import nltk\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import pickle\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "import copy\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "sb.set_style(style=\"whitegrid\")\n",
    "sb.set_color_codes()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81db7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 1201):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2b1f6",
   "metadata": {},
   "source": [
    "## 1️⃣ 데이터 준비\n",
    "> Dacon에서 열린 **`월간 데이콘 신용카드 사기 거래 탐지 AI 경진대회`**의 신용카드 거래 데이터를 사용하겠습니다! (대회에서 제공한 데이터의 원 출처는 대회 데이터 설명 탭에 나와있습니다.) \n",
    "\n",
    "**1. 학습(Train) 데이터셋 (113842개)**  \n",
    "\n",
    "파일명: train.csv  \n",
    "설명: 정상, 사기 거래의 여부를 알 수 없는(대부분 정상 거래) 신용 카드 데이터 (Unlabeled)  \n",
    "ID : 신용 카드 거래 ID  \n",
    "Column ('V1', 'V2', 'V3', ... ,'V30) : 비식별화된 신용 카드 거래 Feature  \n",
    "\n",
    "\n",
    "**2. 검증(Validation) 데이터셋 (28462개)**   \n",
    "\n",
    "파일명: val.csv  \n",
    "설명: 정상, 사기 거래의 여부가 포함된 신용 카드 데이터\n",
    "ID : 신용 카드 거래 ID   \n",
    "Column ('V1', 'V2', 'V3', ... ,'V30) : 비식별화된 신용 카드 거래 Feature  \n",
    "Class : 신용 카드 거래의 정상, 사기 여부 (정상 : 0, 사기 : 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af074c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"./data/credit_card_fraud_detection/train.csv\")\n",
    "val_dataset = pd.read_csv(\"./data/credit_card_fraud_detection/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6f3ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.drop(columns=['ID'])\n",
    "val_dataset_label = val_dataset['Class']\n",
    "val_dataset_attributes = val_dataset.drop(columns=['Class', 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5199460b",
   "metadata": {},
   "source": [
    "## 1️⃣ Isolation Forest from Scratch\n",
    "> 코드는 https://github.com/mgckind/iso_forest 에서 가져왔습니다. 다만, 코드에 주석이 없어 알고리즘을 알고 코드를 봐도 이해가 어렵습니다. 그래서 본 목차는 위의 코드를 기반으로 신용카드 거래 데이터셋에 맞는 추가적인 리팩토링 및 pseudo code와 함께 line by line으로 함께 이해하는 것이 목표입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47adb9f",
   "metadata": {},
   "source": [
    "### 🔎 Node\n",
    "> Tree의 각 Node에는 어떤 정보가 포함될까요?\n",
    "- 여기서 `External Node`란 자식 노드가 없는 노드(즉, Leaf Node), `Internal Node`란 적어도 하나의 자식 노드가 있는 노드를 뜻합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f516cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, X, q, p, e, left, right, node_type = ''):\n",
    "        self.e = e  # 현재 tree height\n",
    "        self.size = len(X) # 데이터에 포함된 object 개수\n",
    "        self.q = q  # split attribute\n",
    "        self.p = p  # split point\n",
    "        self.left = left  # 현재 Node의 Left Node\n",
    "        self.right = right  # 현재 Node의 Right Node\n",
    "        self.ntype = node_type  # 현재 Node의 Type (External Node or Internal Node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6655a",
   "metadata": {},
   "source": [
    "### 🔎 iTree\n",
    "<img src=\"images/if_itree.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af799fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class iTree(object):\n",
    "    def __init__(self, X, e, l):\n",
    "        self.e = e  # 현재 tree height\n",
    "        self.X = X  # 데이터\n",
    "        self.size = len(X) # 데이터에 포함된 object 개수\n",
    "        self.Q = list(X.columns)  # 데이터의 attributes\n",
    "        self.l = l  # 종료 조건에 사용될 tree의 height limit\n",
    "        self.p = None  # split point (max(values of q), min(values of q) 중 랜덤하게 선택됨)\n",
    "        self.q = None  # split attribute (Q에서 랜덤하게 선택됨)\n",
    "        self.exnodes = 0  # external node\n",
    "        self.root = self.make_tree(X, e, l)\n",
    "        \n",
    "\n",
    "    def make_tree(self, X, e, l, split_criteria='random'):\n",
    "        self.e = e\n",
    "        \n",
    "        # 종료 조건\n",
    "        # 1. 현재 tree height이 사전 지정한 height limit에 도달한 경우\n",
    "        # 2. leaf node에 데이터가 한개만 남겨진 경우\n",
    "        if e >= l or len(X) <= 1:\n",
    "            left = None\n",
    "            right = None\n",
    "            self.exnodes += 1\n",
    "            return Node(X, self.q, self.p, e, left, right, node_type = 'exNode')\n",
    "        else:\n",
    "            # Split하는 기준이 되는 Attribute를 데이터의 Attributes 중에서 랜덤하게 선택\n",
    "            self.q = random.choice(self.Q)\n",
    "            mini = X[:][self.q].min()\n",
    "            maxi = X[:][self.q].max()\n",
    "            if mini==maxi:\n",
    "                left = None\n",
    "                right = None\n",
    "                self.exnodes += 1\n",
    "                return Node(X, self.q, self.p, e, left, right, node_type = 'exNode' )\n",
    "            \n",
    "            # Split Point를 min(values of q), max(values of q) 중 랜덤하게 선택\n",
    "            self.p = random.uniform(mini, maxi)\n",
    "            \n",
    "            # q가 p보다 작은 데이터들은 left node로, q가 p보다 같거나 큰 데이터들은 right node로 지정\n",
    "            # 위와 같은 과정들을 재귀적으로 분기 진행\n",
    "            w = np.where(X[:][self.q] < self.p, True, False)\n",
    "            return Node(X, self.q, self.p, e,\\\n",
    "                        left=self.make_tree(X[w], e+1, l),\\\n",
    "                        right=self.make_tree(X[~w], e+1, l),\\\n",
    "                        node_type = 'inNode')\n",
    "\n",
    "    def get_node(self, path):\n",
    "        node = self.root\n",
    "        for p in path:\n",
    "            if p == 'L' : node = node.left\n",
    "            if p == 'R' : node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d8529",
   "metadata": {},
   "source": [
    "### 🔎 iForest\n",
    "> 그럼 이제 위에서 구축된 Isolation Tree를 가지고 본격적으로 Forest를 구축해보겠습니다! Isolation Forest란 말그대로 여러 Isolation Tree로 구성된 숲을 뜻합니다. 위에서 확인했다싶이 뭐든지 랜덤하게 선택되고, 랜덤하게 분기하니까 하나의 Tree로 얻어진 Anomaly Score를 사용하면 성능이 좋지 않겠죠..?! 그러므로 여러 Tree를 통해 얻은 Anomaly Score를 최종적으로 평균내서 사용하게 됩니다.\n",
    "\n",
    "\n",
    "<img src=\"images/if_iforest.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5415723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_factor(n) :\n",
    "    return 2.0*(np.log(n-1)+0.5772156649) - (2.0*(n-1.)/(n*1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee49d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class iForest(object):\n",
    "    def __init__(self, X, ntrees, sample_size, limit=None):\n",
    "        self.ntrees = ntrees  # 몇 개의 Tree로 숲을 구성할지 결정\n",
    "        self.X = X\n",
    "        self.nobjs = len(X)\n",
    "        self.sample = sample_size  # 샘플링 사이즈 (몇 개의 데이터를 이용해 각 tree를 학습할지)\n",
    "        self.Trees = []\n",
    "        self.limit = limit  # height limit (iTree를 구축할 때 종료 조건에 활용될 변수)\n",
    "        if limit is None:\n",
    "            self.limit = int(np.ceil(np.log2(self.sample)))\n",
    "        self.c = c_factor(self.sample)\n",
    "        # pseudo code #3 ~ #6\n",
    "        # 사전 설정한 Tree 개수에 따라 각 Tree별 랜덤하게 데이터 샘플링 후, iTree 구축\n",
    "        for i in tqdm(range(self.ntrees), desc=\"Constructing Tree...\"):\n",
    "            ix = random.sample(range(self.nobjs), self.sample)\n",
    "            X_p = X.loc[ix]\n",
    "            self.Trees.append(iTree(X_p, 0, self.limit))\n",
    "\n",
    "    def compute_paths(self, X_in = None):\n",
    "        if X_in is None:\n",
    "            X_in = self.X\n",
    "        S = np.zeros(len(X_in))\n",
    "        for i in  tqdm(range(len(X_in)), desc=\"Computing path...\"):\n",
    "            h_temp = 0\n",
    "            for j in range(self.ntrees):\n",
    "                h_temp += PathFactor(X_in.loc[i], self.Trees[j]).path*1.0\n",
    "            \"\"\"\n",
    "            Anomaly Score 계산식\n",
    "            Eh = 전체 Tree에 대해 특정 Leaf Node(데이터)까지의 평균 길이 \n",
    "            높을수록 데이터를 고립시키는데 많은 분기가 필요 → 정상데이터일 확률 ⬆\n",
    "            낮을수록 데이터를 고립시키는데 적은 분기가 필요 → 이상치일 확률 ⬆\n",
    "            \"\"\"\n",
    "            Eh = h_temp / self.ntrees\n",
    "            S[i] = 2.0**(-Eh/self.c)\n",
    "        return S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbea1e1",
   "metadata": {},
   "source": [
    "### 🔎 Path Length\n",
    "> 이상치 여부를 판단하기 위해선 각 Tree 별 데이터가 고립되기까지의 평균 길이 (즉, 루트 노드로부터 데이터 노드까지의 거리)를 계산해야 합니다.    \n",
    "\n",
    "<img src=\"images/if_pathlength.png\" width=\"450\">\n",
    "\n",
    "\n",
    "> 💡 특히 처음 강의 자료에서 pseudo code만 보았을 땐 #1~#2이 무엇을 위한 건지 이해가 안됐는데 **height limit에 의해 분기가 중단되어 고려되지 못한 실질적인 depth를 반영하기 위해** 각 데이터 별로 c값을 더해주는 거라고 합니다!  \n",
    "\n",
    "> 만약 leaf node에 포함된 데이터가 1개라면 완벽히 고립된 것이므로 c=1로 두고, 1개보다 크다면 height limit으로 인해 더 이상 분기하지 못한 것이니 아래와 같은 식으로 정의합니다. 이렇게 구한 c값을 각 데이터 별 leaf node의 depth에 더해주면 각 데이터 별 평균 길이를 각 Tree별로 보정하는 효과를 낼 수 있습니다.\n",
    "\n",
    "<img src=\"images/if_c.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab1c5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathFactor(object):\n",
    "    def __init__(self, x, itree):\n",
    "        self.path_list=[]        \n",
    "        self.x = x\n",
    "        self.e = 0\n",
    "        self.path = self.find_path(itree.root)\n",
    "\n",
    "    def find_path(self,T):\n",
    "        if T.ntype == 'exNode':\n",
    "            # Case 1 : leaf node에 데이터가 한개만 존재하는 완전히 고립된 상태\n",
    "            # Current Tree Path를 그대로 반화\n",
    "            if T.size == 1: return self.e\n",
    "            # Case 2 : leaf node에 데이터가 두개 이상 존재하는 완전히 고립되지 못한 상태\n",
    "            # Current Tree Path + c_factor 반환\n",
    "            else:\n",
    "                self.e = self.e + c_factor(T.size)\n",
    "                return self.e\n",
    "        else:\n",
    "            a = T.q\n",
    "            self.e += 1\n",
    "            if self.x[a] < T.p:\n",
    "                self.path_list.append('L')\n",
    "                return self.find_path(T.left)\n",
    "            else:\n",
    "                self.path_list.append('R')\n",
    "                return self.find_path(T.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d40599",
   "metadata": {},
   "source": [
    "### 🔎 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75356797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(anomaly_score):\n",
    "    # Anomaly Distribution을 시각화\n",
    "    f, axes = plt.subplots(1, 1, figsize=(7, 7), sharex=True)\n",
    "    sb.distplot(anomaly_score, kde=True, color=\"b\", ax=axes, axlabel='anomaly score')\n",
    "        \n",
    "def show_result(anomaly_score, threshold=[0.5, 0.6, 0.7, 0.8, 0.9]):\n",
    "    f1_scores = {}\n",
    "    for t in threshold:\n",
    "        result = np.where(anomaly_score >= t, 1, 0)\n",
    "        f1_scores[t] = round(f1_score(val_dataset_label, result, average='macro'), 3)\n",
    "    for t in f1_scores.keys():\n",
    "        print(f'Threshold: {t} => Macro F1-Score: {f1_scores[t]}')\n",
    "    print(f'⭐️ Best F1-Score : {sorted(f1_scores.items(), reverse=True, key=lambda item: item[1])[0][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27262c84",
   "metadata": {},
   "source": [
    "- **tree 개수에 따른 성능 변화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2c2faef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Number of 🌳 : 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3934037546cd45b18786e5c692479163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Constructing Tree...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed234cb1629464a84e263b69c76d538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing path...:   0%|          | 0/28462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.5 => Macro F1-Score: 0.511\n",
      "Threshold: 0.6 => Macro F1-Score: 0.566\n",
      "Threshold: 0.7 => Macro F1-Score: 0.648\n",
      "Threshold: 0.8 => Macro F1-Score: 0.5\n",
      "Threshold: 0.9 => Macro F1-Score: 0.5\n",
      "⭐️ Best F1-Score : 0.648\n",
      "====================================================================================================\n",
      "Number of 🌳 : 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf04bdb99f64780805fcdb8a1d7b477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Constructing Tree...:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3621cf0d07a4455b9493094676348ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing path...:   0%|          | 0/28462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.5 => Macro F1-Score: 0.508\n",
      "Threshold: 0.6 => Macro F1-Score: 0.561\n",
      "Threshold: 0.7 => Macro F1-Score: 0.545\n",
      "Threshold: 0.8 => Macro F1-Score: 0.5\n",
      "Threshold: 0.9 => Macro F1-Score: 0.5\n",
      "⭐️ Best F1-Score : 0.561\n",
      "====================================================================================================\n",
      "Number of 🌳 : 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb04284c80964c06b03f4f5a4a3becb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Constructing Tree...:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34032369f23b49f392de3ec2152504d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing path...:   0%|          | 0/28462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.5 => Macro F1-Score: 0.506\n",
      "Threshold: 0.6 => Macro F1-Score: 0.558\n",
      "Threshold: 0.7 => Macro F1-Score: 0.552\n",
      "Threshold: 0.8 => Macro F1-Score: 0.5\n",
      "Threshold: 0.9 => Macro F1-Score: 0.5\n",
      "⭐️ Best F1-Score : 0.558\n"
     ]
    }
   ],
   "source": [
    "exp_ntrees = [10, 50, 100]\n",
    "exp_ntrees_results = {}\n",
    "exp_ntrees_forests = []\n",
    "for ntree in exp_ntrees:\n",
    "    print(\"=\"*100)\n",
    "    print(f'Number of 🌳 : {ntree}')\n",
    "    F = iForest(train_dataset, ntrees=ntree, sample_size=256)\n",
    "    S_val = F.compute_paths(X_in=val_dataset_attributes)\n",
    "    exp_ntrees_results[ntree] = S_val\n",
    "    exp_ntrees_forests.append(F)\n",
    "    show_result(S_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9f00cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adfadsfadf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56348/3235631355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madfadsfadf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'adfadsfadf' is not defined"
     ]
    }
   ],
   "source": [
    "adfadsfadf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a3121",
   "metadata": {},
   "source": [
    "- **sample size에 따른 성능 변화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d7b7176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Number of Sample Size : 128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711925b4facf4d4c9589febe92646e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Constructing Tree...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f975429d97024c54a562008bf3b5cc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing path...:   0%|          | 0/28462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.5 => Macro F1-Score: 0.483\n",
      "Threshold: 0.6 => Macro F1-Score: 0.519\n",
      "Threshold: 0.7 => Macro F1-Score: 0.499\n",
      "Threshold: 0.8 => Macro F1-Score: 0.5\n",
      "Threshold: 0.9 => Macro F1-Score: 0.5\n",
      "⭐️ Best F1-Score : 0.519\n",
      "====================================================================================================\n",
      "Number of Sample Size : 256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9227d52860aa4311ad1ee8c3fd1b4202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Constructing Tree...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f5942d75fe4efca7c452aad2d4a588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing path...:   0%|          | 0/28462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.5 => Macro F1-Score: 0.5\n",
      "Threshold: 0.6 => Macro F1-Score: 0.554\n",
      "Threshold: 0.7 => Macro F1-Score: 0.614\n",
      "Threshold: 0.8 => Macro F1-Score: 0.5\n",
      "Threshold: 0.9 => Macro F1-Score: 0.5\n",
      "⭐️ Best F1-Score : 0.614\n",
      "====================================================================================================\n",
      "Number of Sample Size : 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ec398b81f04249b539fa292ad35939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Constructing Tree...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad9bb4551ce4d65b8ba4116e55edf63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing path...:   0%|          | 0/28462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.5 => Macro F1-Score: 0.515\n",
      "Threshold: 0.6 => Macro F1-Score: 0.599\n",
      "Threshold: 0.7 => Macro F1-Score: 0.707\n",
      "Threshold: 0.8 => Macro F1-Score: 0.5\n",
      "Threshold: 0.9 => Macro F1-Score: 0.5\n",
      "⭐️ Best F1-Score : 0.707\n"
     ]
    }
   ],
   "source": [
    "exp_ss = [128, 256, 512]\n",
    "exp_ss_results = {}\n",
    "exp_ss_forests = []\n",
    "for ss in exp_ss:\n",
    "    print(\"=\"*100)\n",
    "    print(f'Number of Sample Size : {ss}')\n",
    "    F = iForest(train_dataset, ntrees=10, sample_size=ss)\n",
    "    S_val = F.compute_paths(X_in=val_dataset_attributes)\n",
    "    exp_ss_results[ss] = S_val\n",
    "    exp_ss_forests.append(F)\n",
    "    show_result(S_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8449c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b9e21e",
   "metadata": {},
   "source": [
    "## 2️⃣ Non-random Split using Information gain\n",
    "> Information gain을 통해 Decision Tree를 구축하기 위해서 필요한 Label 정보가 현재 Train Dataset에는 없으므로 한번 학습을 진행한 Isolation Forest를 통해 Pseudo Label을 구축하겠습니다.\n",
    "- Reference\n",
    "    - https://community.rapidminer.com/discussion/58166/decision-tree-without-a-label-is-it-possible\n",
    "    - https://towardsdatascience.com/entropy-and-information-gain-in-decision-trees-c7db67a3a293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fad4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo_label(dataset):\n",
    "    # 가장 성능이 좋았던 n_tree=10, sample_size=512의 Isolation Forest를 사용하겠습니다.\n",
    "    S_train = exp_ss_forests[-1].compute_paths(X_in=train_dataset)\n",
    "    pseudo_label = np.where(S_train >= 0.7, 1, 0)\n",
    "    return pseudo_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c81604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_copy = train_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58709599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c945a5b50f400ebd67652e1f560e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing path...:   0%|          | 0/113842 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_copy['pseudo_label'] = get_pseudo_label(train_dataset_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c6cbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_label = train_dataset_copy['pseudo_label']\n",
    "train_dataset_attributes = train_dataset_copy.drop(columns=['pseudo_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db510fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(column):\n",
    "    # Compute the counts of each unique value in the column\n",
    "    counts = np.bincount(column)\n",
    "    # Divide by the total column length to get a probability\n",
    "    probabilities = counts / len(column)\n",
    "    \n",
    "    # Initialize the entropy to 0\n",
    "    entropy = 0\n",
    "    # Loop through the probabilities, and add each one to the total entropy\n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            # use log from math and set base to 2\n",
    "            entropy += prob * math.log(prob, 2)\n",
    "    \n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_information_gain(data, split_name, target_name='pseudo_label'):\n",
    "    # Calculate the original entropy\n",
    "    original_entropy = calc_entropy(data[target_name])\n",
    "    \n",
    "    #Find the unique values in the column\n",
    "    values = data[split_name].unique()\n",
    "    \n",
    "    # Make two subsets of the data, based on the unique values\n",
    "    left_split = data[data[split_name] == values[0]]\n",
    "    right_split = data[data[split_name] == values[1]]\n",
    "    \n",
    "    # Loop through the splits and calculate the subset entropies\n",
    "    to_subtract = 0\n",
    "    for subset in [left_split, right_split]:\n",
    "        prob = (subset.shape[0] / data.shape[0]) \n",
    "        to_subtract += prob * calc_entropy(subset[target_name])\n",
    "    \n",
    "    # Return information gain\n",
    "    return original_entropy - to_subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_info_gain(columns):\n",
    "    #Intialize an empty dictionary for information gains\n",
    "    information_gains = {}\n",
    "\n",
    "    #Iterate through each column name in our list\n",
    "    for col in columns:\n",
    "        #Find the information gain for the column\n",
    "        information_gain = calc_information_gain(midwest, col, \n",
    "        #Add the information gain to our dictionary using the column name as the ekey                                         \n",
    "        information_gains[col] = information_gain\n",
    "\n",
    "    #Return the key with the highest value                                          \n",
    "    return max(information_gains, key=information_gains.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb954b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class iTree(object):\n",
    "    def __init__(self, X, e, l):\n",
    "        self.e = e  # 현재 tree height\n",
    "        self.X = X  # 데이터\n",
    "        self.size = len(X) # 데이터에 포함된 object 개수\n",
    "        self.Q = np.arange(np.shape(X)[1], dtype='int') # 데이터의 attributes\n",
    "        self.l = l  # 종료 조건에 사용될 tree의 height limit\n",
    "        self.p = None  # split point (max(values of q), min(values of q) 중 랜덤하게 선택됨)\n",
    "        self.q = None  # split attribute (Q에서 랜덤하게 선택됨)\n",
    "        self.exnodes = 0  # external node\n",
    "        self.root = self.make_tree(X, e, l)\n",
    "        \n",
    "\n",
    "    def make_tree(self, X, e, l, split_criteria='random'):\n",
    "        self.e = e\n",
    "        \n",
    "        # 종료 조건\n",
    "        # 1. 현재 tree height이 사전 지정한 height limit에 도달한 경우\n",
    "        # 2. \n",
    "        if e >= l or len(X) <= 1:\n",
    "            left = None\n",
    "            right = None\n",
    "            self.exnodes += 1\n",
    "            return Node(X, self.q, self.p, e, left, right, node_type = 'exNode')\n",
    "        else:\n",
    "            # Split하는 기준이 되는 Attribute를 데이터의 Attributes 중에서 랜덤하게 선택\n",
    "            self.q = random.choice(self.Q)\n",
    "            mini = X[:,self.q].min()\n",
    "            maxi = X[:,self.q].max()\n",
    "            if mini==maxi:\n",
    "                left = None\n",
    "                right = None\n",
    "                self.exnodes += 1\n",
    "                return Node(X, self.q, self.p, e, left, right, node_type = 'exNode' )\n",
    "            \n",
    "            # Split Point를 min(values of q), max(values of q) 중 랜덤하게 선택\n",
    "            self.p = random.uniform(mini, maxi)\n",
    "            \n",
    "            # q가 p보다 작은 데이터들은 left node로, q가 p보다 같거나 큰 데이터들은 right node로 지정\n",
    "            # 위와 같은 과정들을 재귀적으로 분기 진행\n",
    "            w = np.where(X[:, self.q] < self.p, True, False)\n",
    "            return Node(X, self.q, self.p, e,\\\n",
    "                        left=self.make_tree(X[w], e+1, l),\\\n",
    "                        right=self.make_tree(X[~w], e+1, l),\\\n",
    "                        node_type = 'inNode')\n",
    "\n",
    "    def get_node(self, path):\n",
    "        node = self.root\n",
    "        for p in path:\n",
    "            if p == 'L' : node = node.left\n",
    "            if p == 'R' : node = node.right\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5e1b4",
   "metadata": {},
   "source": [
    "> 유튜브 영상을 제작하면서 들었던 궁금증을 해결할 수 있었던 유익한 시간이었습니다. 그럼 다음 튜토리얼에 **`Ensemble Learning`**으로 다시 찾아뵙겠습니다.\n",
    "\n",
    "<img src=\"images/9_turtle.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86e6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
